{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## REI602M Machine Learning - Homework 10\n",
    "### Due: *Thursday* 28.3.2019\n",
    "\n",
    "**Objectives**: Convolutional neural networks (CNNs), Recurrent neural networks (RNNs)\n",
    "\n",
    "**Name**: (your name here), **email: ** (your email here), **collaborators:** (if any)\n",
    "\n",
    "**Notes**: You need TensorFlow for this assignment. Refer to installation instructions on Piazza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. [Image classification with CNNs, 50 points] The CIFAR-10 is a small image classification dataset consisting of 60000 color images of size 32x32 in 10 classes.\n",
    "<img src=\"./cifar-10-classes.png\" width=\"500\"/>\n",
    "Human accuracy on CIFAR-10 is approximately 94% while state of the art CNNs achieve around 99% accuracy! You can expect accuracy close to 80% in this problem and around 84% in problem 2 (this is comparable to state of the art performance in 2013). \n",
    "\n",
    "The data is split into training and validation sets below. There is no separate test set so the accuracy estimates that you obtain will be somewhat optimistic. Starting from a simple network architecture you gradually add layers, with the aim of improving accuracy. In each of the tasks below, you report the final training and validation accuracies and provide a graph showing how they change during training. What can you conclude from the graph in each case? Monitor the accuracy during training and stop when the validation accuracy no longer improves. Ten epochs should be sufficient in most cases.\n",
    "\n",
    "In the following, INPUT denotes the input layer, FC denotes a fully connected layer, CONV-$m$ represents a 2D-convolutional layer with $m$ filters, POOL corresponds to a 2D pooling layer, RELU to ReLU activation units, [...]\\*n denotes repetition $n$ times of the units inside the brackets. The last last layer (FCS) denotes a fully connected layer with 10 nodes and softmax activation (this is the classification step). Use dropout for regulatization and only following FC layers.\n",
    "\n",
    "a) INPUT -> [FC -> RELU] -> FCS (conventional feedforward network)\n",
    "\n",
    "b) INPUT -> [CONV-32 -> RELU -> POOL] -> FCS (minimalistic CNN)\n",
    "\n",
    "c) INPUT -> [CONV-32 -> RELU]\\*2 -> POOL]\\*2 -> [FC -> RELU]\\*1 -> FCS\n",
    "\n",
    "d) [CONV-32 -> RELU]\\*2 -> POOL -> [CONV-64 -> RELU]\\*2 -> POOL -> [CONV-128 -> RELU]\\*3 -> POOL -> FC -> FCS (simplified VGGnet)\n",
    "\n",
    "*Comments*:\n",
    "* Implement your networks using Keras. You can see examples of fully connected networks in `v10_nn_keras.ipynb` and a convolutional network in `v11_dnn.ipynb`.\n",
    "* Regularization of convolutional layers does not seem to be very effective. Fully connected need regularization to prevent overfitting. Dropout with $p=0.5$ is usually quite effective.\n",
    "* Use `padding=\"same\"` to zero-pad the input to convolutional layers.\n",
    "* You can continue training a model by calling `model.fit` repeatedly.\n",
    "* To save a model use `model.save(filename)`. You may also want to look into model checkpoints and early stopping. See `ModelCheckpoint` and `EarlyStopping` in the Keras documentation.\n",
    "* The CIFAR-10 \"high score\" was obtained by training giant deep networks on huge image databases in order to learn feature maps relevant to image classification. The networks were then fine-tuned on CIFAR-10 (this an example of *transfer learning*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 data set of tiny images (~170 MB)\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # Takes considerable time first time around\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "num_classes=len(np.unique(y_train))\n",
    "print(\"Number of classes:\", num_classes)\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Convert to 32-bit floats\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Scale data\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# THINK:\n",
    "x_val = x_test\n",
    "y_val = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. [Data augmentation, 20 points] When the amount of training data is small in relation to the number of parameters in a model, overfitting becomes an issue. In many specialized image recognition tasks such as tumor classification, the amount of labeled data is often quite limited and a state of the art convolutional network are likely to severly overfit the data set. Data augmentation refers to techniques that create additional training examples from the original data set. For image data it is possible to create additional training examples by simple operations such as reflection, cropping and translation as well as by changing the color palette.\n",
    "\n",
    "Take the best network from problem 1) and perform image augmention *during* training using the `ImageDataGenerator` class in Keras. You may need to train for more than 10 epochs. Report your results in the same way as you did in problem 1). Comment briefly on the type of mistakes that your network makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. [Learning to perform subtraction with an RNN, 30 points] Take a close look at the recurrent neural network code shown in class (https://keras.io/examples/addition_rnn/). It shows how sequence to sequence learning can be used to learn addition of small numbers. This is done by presenting the network with input-output pairs, where the input is a string on the form \"123+456\" and the output is \"579\".\n",
    "\n",
    "Modify the code for addition so that the network learns *subtraction* of three digit numbers. What is the validation accuracy after 30 epochs? You should mark all the modifications that you do to the code by inserting comments on the form\n",
    "```python\n",
    "# START MOD\n",
    "...\n",
    "# END MOD\n",
    "```\n",
    "\n",
    "*Comments*:\n",
    "* You need to modify the test that prevents the same example occurring multiple times in the list of \"questions\" (addition is commutative but subtraction is not).\n",
    "* `val_acc` does not represent the fraction of correctly predicted validation examples (I have no idea why!) You need to write a short piece of code that sends all the examples in the validation set through the network and counts the number of mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
