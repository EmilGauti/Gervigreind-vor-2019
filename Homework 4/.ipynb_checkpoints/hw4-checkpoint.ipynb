{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 4\n",
    "### Due: Sunday 10.2.2019\n",
    "\n",
    "**Objectives**: Support vector machines, feature selection. \n",
    "\n",
    "**Name**: Emil Gauti Fri√∞riksson, **email: ** egf3@hi.is, **collaborators:** (if any)\n",
    "\n",
    "Please provide your solutions by filling in the appropriate cells in this notebook, creating new cells as needed. Hand in your solution on Gradescope, taking care to locate the appropriate page numbers in the PDF document. Make sure that you are familiar with the course rules on collaboration (encouraged) and copying (very, very, bad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) [Stochastic gradient descent for SVM, 40 points]. In this problem you are to implement a stochastic gradient descent algorithm for training a linear SVM. The model is $f_\\theta(x)=\\theta^T x$ (to include an intercept term you can simply set $x_0=1$ as before). The algorithm minimizes the SVM objective function\n",
    "$$\n",
    "   J(\\theta) = \\frac{\\lambda}{2}\\theta^T \\theta + \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1-y^{(i)}\\theta^T x^{(i)}).\n",
    "$$\n",
    "The hinge loss $\\max(0, 1-z)$ is not differentiable at $z=1$ and this results in a objective function which is not differentiable everywhere, hence the gradient of $J(\\theta)$ is not defined everywhere. To deal with this, the SGD algorithm uses the *sub-gradient* of $J$ instead (see below). The algorithm starts from $\\theta^{(0)}=0$ and performs a fixed number of iterations, with step $k$ as follows:\n",
    "\n",
    "Select $i$ uniformly at random from $[1,n]$\n",
    "\n",
    "$\\alpha^{(k)} = \\frac{1}{\\lambda k}$\n",
    "\n",
    "if $y^{(i)}~(\\theta^{(k)})^T x^{(i)} < 1 ~\\textrm{then}$\n",
    "\n",
    "$\\quad \\theta^{(k+1)} = \\theta^{(k)} - \\alpha^{(k)}(\\lambda \\theta^{(k)} - y^{(i)} x^{(i)})$\n",
    "\n",
    "else\n",
    "\n",
    "$\\quad \\theta^{(k+1)} = \\theta^{(k)} - \\alpha^{(k)} \\lambda \\theta^{(k)}$\n",
    "\n",
    "where $\\theta^{(k)}$ denotes the parameter *vector* in iteration $k$ and $\\lambda>0$ is a regularization hyper-parameter. The step size $\\alpha^{(k)}$ decays over the course of iterations instead of being constant as we've seen in previous implementations of SGD.\n",
    "\n",
    "a) [20 points] Implement the SGD algorithm above and use it to classify dataset `hw3_data_a` from homework 3 using $\\lambda=1/100$. Report the model coefficients and training set accuracy.\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) To sample uniformly at random from $[0,n-1]$ use `np.random.randint`.\n",
    "\n",
    "2). A *sub-gradient* is a generalization of the gradient for convex functions which are not necessarily differentiable. Such functions arise quite frequently in machine learning, e.g. when one is using the 1-norm for regularization. The sub-gradient of a function at a point is the slope of *a* hyperplane that passes through the point and lies below the graph of the function. We will not cover sub-gradients in more detail in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.3033033   3.18615366  2.53272733]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,) (100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-307803f9ded1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX2_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'^'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Insert code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (100,) "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def svm_sgd(X,y,lambda_par,max_epochs=10):\n",
    "    # X ... Input variables (n x p matrix)\n",
    "    # y ... Labels (n vector), -1 or 1\n",
    "    # lambda_par ... Regularization constant (non-negative)\n",
    "    # max_epochs ... Maximum number of \"passes\" through the data set\n",
    "\n",
    "    assert(lambda_par > 0)\n",
    "    max_iter = max_epochs*X.shape[0]\n",
    "    theta = np.zeros(3)\n",
    "    for k in range(1,max_iter):\n",
    "        i = np.random.randint(X.shape[0])\n",
    "        alpha = 1/(lambda_par*k)\n",
    "        if y[i]*theta.T@X[i,:]<1:\n",
    "            theta = theta - alpha*(lambda_par*theta-y[i]*X[i,:])\n",
    "        else:\n",
    "            theta = theta - alpha*lambda_par*theta\n",
    "    return theta\n",
    "    \n",
    "    \n",
    "\n",
    "y,X1,X2 = np.loadtxt('hw3_data_a.txt',unpack=True)\n",
    "X = np.c_[np.ones(len(X1)),X1,X2]\n",
    "theta = svm_sgd(X,y,1/100,10)\n",
    "print(theta)\n",
    "#print(X)\n",
    "X1_m = X1[np.where(y == -1)]\n",
    "X1_p = X1[np.where(y == 1)]\n",
    "X2_m = X2[np.where(y == -1)]\n",
    "X2_p = X2[np.where(y == 1)]\n",
    "plt.scatter(X1_m,X2_m, marker='+')\n",
    "plt.scatter(X1_p,X2_p, marker='^')\n",
    "z = np.linspace(0,1,100)\n",
    "f = theta*z\n",
    "plt.plot(z,f)\n",
    "    # Insert code here\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) [20 points] Train a linear SVM classifier on a subset of the MNIST dataset from homework 3 consisting of digits 5 and 8 only. Keep track of the objective function value and training set error during the course of iterations. \n",
    "\n",
    "i) Plot the objective function values and training set error as a function of iteration number. Consider using a log-scale for iteration and/or objective values (see e.g. `matplotlib.pyplot.loglog`).\n",
    "\n",
    "ii) Evaluate classifier accuracy using the test set (digits 5 and 8 only) using your classifier.\n",
    "\n",
    "iii) Compare the accuracy of your classifier with the SVM implementation in `sklearn.svm.LinearSVC`.\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) Computing the function values and training set error requires a pass through all the training data. This is computationally expensive, especially for large data sets, so you should compute these values once every $T$ iterations where $T$ could e.g. be 100, 1000 or $n$.\n",
    "\n",
    "2) To speed up computations use matrix and vector operations instead of *for*-loops where possible. For example, if the training set is in matrix $X$ you can classify all the examples in a single matrix-vector multiplication, $y_{pred}=X\\theta$. This issue is discussed in some detail in http://cs229.stanford.edu/section/vec_demo/Vectorization_Section.pdf\n",
    "\n",
    "3) You can use the following code to discard all but classes 5 and 8 with `x_train, y_train = filter_classes(x_train, y_train, 5, 8)`\n",
    "\n",
    "```python\n",
    "def filter_classes(X, y, cls1, cls2):\n",
    "    # Reduces training set (X,y) so that it contains only labels cls1, cls2 and then maps the labels to {-1,1}\n",
    "    cond = np.logical_or(y == cls1, y == cls2) \n",
    "    X_red = X[cond,:]\n",
    "    y_red = [-1 if c==cls1 else 1 for c in y[cond]]\n",
    "    return X_red, y_red```\n",
    "    \n",
    "4) Note that the $\\lambda$ parameter in the above SVM formulation is related to the $C$ parameter in the \"standard\" SVM formulation via $\\lambda=1/(nC)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) [Spam filtering, 30 points - This is based on a problem from Andrew Ng's machine learning course at Stanford]\n",
    "In recent years, spam on electronic newsgroups has been an increasing problem. Here, you will\n",
    "build a classifier to distinguish between \"real\" newsgroup messages, and spam messages.\n",
    "For this experiment, a set of spam emails and a set of genuine newsgroup messages have been obtained.\n",
    "Using only the subject line and body of each message, we‚Äôll learn to distinguish\n",
    "between the spam and non-spam.\n",
    "All the files for the problem are in the file `email_spam.zip`.\n",
    "In order to get the text emails into a form usable by a off-the shelf classifier, some preprocessing on the\n",
    "messages has already been performed. You can look at two sample spam emails in the files `spam_sample_original`,\n",
    "and their preprocessed forms in the files `spam_sample_preprocessed*`. The first line in\n",
    "the preprocessed format is just the label and is not part of the message. The preprocessing\n",
    "ensures that only the message body and subject remain in the dataset; email addresses\n",
    "(EMAILADDR), web addresses (HTTPADDR), currency (DOLLAR) and numbers (NUMBER)\n",
    "were also replaced by the special tokens to allow them to be considered properly in the\n",
    "classification process. (In this problem, we‚Äôll going to call the features \"tokens\" rather than\n",
    "\"words,\" since some of the features will correspond to special values like EMAILADDR.\n",
    "You don‚Äôt have to worry about the distinction.) The files `news_sample original` and\n",
    "`news_sample_preprocessed` also give an example of a non-spam mail.\n",
    "\n",
    "The work to extract feature vectors (i.e. classifier inputs) out of the documents has also been done for you, so you\n",
    "can just load in the design matrices (called document-word matrices in text classification)\n",
    "containing all the data. In a document-word matrix, the $i$-th row represents the $i$-th document/email,\n",
    "and the $j$-th column represents the $j$-th distinct token. Thus, the $(i,j)$-entry of\n",
    "this matrix represents the number of occurrences of the $j$-th token in the $i$-th document.\n",
    "\n",
    "For this problem, we‚Äôve chosen as our set of tokens considered (that is, as our vocabulary)\n",
    "only the medium frequency tokens. The intuition is that tokens that occur too often or\n",
    "too rarely do not have much classification value. (Examples tokens that occur very often\n",
    "are words like \"the\", \"and\", and \"of\", which occur in so many emails and are sufficiently\n",
    "content-free that they aren‚Äôt worth modeling.) Also, words were stemmed using a standard\n",
    "stemming algorithm; basically, this means that ‚Äúprice,‚Äù ‚Äúprices‚Äù and ‚Äúpriced‚Äù have all been\n",
    "replaced with ‚Äúprice,‚Äù so that they can be treated as the same word. For a list of the tokens\n",
    "used, see the variable file `tokenlist`.\n",
    "Since the document-word matrix is extremely sparse (has lots of zero entries), we have\n",
    "stored it in our own efficient format to save space. You don‚Äôt have to worry about this\n",
    "format. The file `read_spam_data.py` provides the function `read_matrix` to read in the document-word\n",
    "matrix and labels.\n",
    "\n",
    "a) [15 points] Train a linear SVM on this dataset using the implementation in `sklearn.svm.LinearSVC`\n",
    "and $C=0.1$. Evaluate the accuracy on the test set for training sets of size 50, 100,\n",
    "200, 400, 800 and 1400 and for the full test set as well.\n",
    "\n",
    "*Comment*: To read the training and test data and the list of tokens behind the features use\n",
    "```python\n",
    "    trainMatrix, tokenlist, trainCategory = readMatrix('MATRIX.TRAIN')\n",
    "    testMatrix, tokenlist, testCategory = readMatrix('MATRIX.TEST')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) [15 points] Classifier accuracy is in general strongly dependent on the selection of inputs to the classifier. The GIGO principle (garbage in, garbage out) usually applies. While some classifiers are quite tolerant towards noisy/irrelevant inputs (e.g. tree-based classifiers), the performance of other classifiers can degrade quickly (e.g. nonlinear SVMs). In the *feature selection* problem the task is to identify which features are most relevant for a given classification problem. By performing a careful selection of features, the performance of a classifier can often be improved significantly. Alternatively, it can be interesting to identify a minimal set of features for acceptable performance (e.g. due to high costs of collecting/measuring the full feature set).\n",
    "\n",
    "A simple feature selection strategy considers the weights in a linear SVM after training has been performed. The larger $|\\theta_k|$ is, the larger the role of the corresponding feature in the decision function. The strategy is therefore to rank the features according to $|\\theta_k|$.\n",
    "\n",
    "i) Train a linear SVM on the full spam data set and list the 10 tokens most important tokens in the above sense.\n",
    "\n",
    "ii) Retrain a Linear SVM using only the most important 20 features/tokens. How does the accuracy compare with the full classifier?\n",
    "\n",
    "*Comment*: The weights are stored in the `coef_` attribute in the LinearSVC class. The above feature selection method is discussed in http://proceedings.mlr.press/v3/chang08a/chang08a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) [SVM classifier with RBF kernel, 30 points] Train an SVM with RBF kernel on the MNIST data set from last week. Use the validation set to select optimal values of $C$ and the RBF parameter $\\gamma$ by performing a \"grid search\" on the validation set. This is simply a double for-loop where the outer loop iterates over one parameter and the inner loop over the other. For each $(C,\\gamma)$ pair train an RBF SVM on a random subset of the training data (e.g. with 5000 samples, otherwise the training time may become prohibitively long) and evaluate accuracy on the validation set. Use the the following values for $C$, [1, 10, 100] and [1/10, 1/100, 1/1000] for $\\gamma$. Use the best $(C,\\gamma)$ pair to train a final classifier, using as much of the training data as you can and report the test set error. How does the performance compare to last week's $k$-NN and logistic regression classifiers?\n",
    "\n",
    "*Comment*: Use the `sklearn.svm.SVC` implementation in scikit-learn. The training is time consuming since multiclass problems are handled in a one-against-one scheme which results in (10)(9)/2=45 binary classifiers that need to be trained. Since we have a separate validation set we avoid performing $k$-fold cross-validation on all 45 classifiers, with $k$ typically 5 or 10 this could increase the training time 10-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
