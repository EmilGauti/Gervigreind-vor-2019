{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 5\n",
    "### Due: *Monday* 18.2.2019\n",
    "\n",
    "**Objectives**: Feature scaling, data pre-processing, parameter selection, tree classifiers\n",
    "\n",
    "**Name**: (your name here), **email: ** (your email here), **collaborators:** (if any)\n",
    "\n",
    "Please provide your solutions by filling in the appropriate cells in this notebook, creating new cells as needed. Hand in your solution on Gradescope, taking care to locate the appropriate page numbers in the PDF document. Make sure that you are familiar with the course rules on collaboration (encouraged) and copying (very, very, bad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. [Pre-processing and parameter tuning in a nonlinear SVM classifier, 30 points] The Statlog Satimage data set is an old benchmark in machine learning. It contains data from satellite images and the aim is to predict land type (red soil, cotton crop etc). There are 36 integer valued features and 6 classes. The file `sat.trn` contains 4435 training examples and `sat.tst` contains 2000 test examples. Your task is to obtain an SVM classifier which achieves high classification accuracy on this data set. Use Scikit-learn to carry out the tasks below.\n",
    "\n",
    "i) [6 points] Evaluate the accuracy of an SVM with an RBF kernel (RBF-SVM) using the default values for $C$ and $\\gamma$.\n",
    "\n",
    "ii) [9 points] Scale the data prior to training an RBF-SVM (make sure to scale the test data accordingly, see lecture notes from week 2) and repeat the task from i).\n",
    "\n",
    "iii) [12 points] Use grid search in combination with cross-validation on the (scaled) training set to identify good values of $C$ and $\\gamma$. Retrain a classifier using the best parameter values on the whole training set and evaluate the accuracy of the resulting classifier. You should start with a coarse grid to identify a range of good values and then do another run with a finer grid.\n",
    "\n",
    "iv) [3 point] Comment briefly on the importance of data scaling and model selection for nonlinear SVMs.\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) At the time of publication (mid 90's) the best performing classifier was a k-nearest neighbor classifier using Euclidean distance which had an error rate of 9.4%. Description of the data set: https://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)\n",
    "\n",
    "2) Use `preprocessing.StandardScaler` for data scaling.\n",
    "\n",
    "3) The procedure of selecting values of hyper-parameters such as $C$ and $\\gamma$ is called *model selection*. *Grid search* refers to a parameter search where model performance is evaluated over a fixed set of parameter values. The parameter values are frequently logarithmically spaced. The performance is often evaluated using cross-validation or a separate vaidation set if that is available. The performance measure here is classifier accuracy, the fraction of correctly classified examples.\n",
    "\n",
    "4) You can use the `GridSearchCV` class in scikit-learn. See sections 3.1 (cross-validation: evaluating estimator performance), 3.2 (tuning the hyper-parameters of an estimator) and the \"Parameter estimation using grid search with cross-validation\" example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. [Feature selection and chemical informatics, 30 points] Quantitative Structure - Activity Relationship (QSAR) models relate the activity of chemical compounds to their structural properties. The activity can e.g. represent the potency of a drug or its toxicity. The structural properties may contain basic information such as i) the fraction of carbon atoms in the compound, ii) the spatial arrangement of atoms in the compound and iii) quantities computed from quantum mechanical simulations (*ab-initio* calculations).\n",
    "\n",
    "The QSAR biodegradation Data Set `biodeg.csv` contains 41 molecular descriptors for two groups of compounds, those that are readily biodegradable (RB) and those that are not (NRB). The data set has 356 examples in the RB class and 699 in the NRB class, i.e. it is somewhat unbalanced.\n",
    "\n",
    "a) [15 points] Using scikit-learn, obtain a Random forests classifier for predicting whether a given compound is readily biodegradable or not. Use a random 80/20 train/test set split for evaluting the performance of your classifier. Report the sensitivity and specificity of the classifier along with accuracy (see below).\n",
    "\n",
    "b) [15 points] List the names of the 10 *most useful* features for the classification task. Retrain a Random forests classifier using only the 10 most useful features and report sensitivity, specificity and accuracy.\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) A description of the data set can be found here: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation\n",
    "\n",
    "2) A correctly predicted RB example is said to be a *true positive* and a correctly predicted NRB examples is said to be a *true negative*. When an NRB example is incorrectly predicted as RB it is a *false positive*. False negatives are defined analogously.\n",
    "\n",
    "The *sensitivity* of a binary classifier is defined as TP/(TP+FN) and the *specificity* is defined as TN/(TN+FP) where TP is the number of true positives etc. These values are conveniently obtained from a confusion matrix.\n",
    "\n",
    "3) Sidenote: Repeatedly retraining a classifier with smaller and smaller number of top features until the out-of-bag error starts to increase can cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. [40 points] In this problem you will construct a predictive model for Telemarketing. The data comes from a telemarketing campaign in Portugal where the goal was to get clients to subscribe to long-term savings deposits. The predictive model is to be used to identify customers that are likely to subscribe, based on personal information, economic indicators, whether the client has been contacted before etc. This should make the campaign more effective and reduce marketing costs.\n",
    "\n",
    "The data is in file `bank-additional-full.csv` with a brief description in `bank-additional-names.txt`. The data contains a mixture of continuous and categorical features, with categorical data in text format. Some preprocessing is therefore needed before you can use it with scikit-learn algorithms.\n",
    "\n",
    "The data is time ordered which means that randomly splitting it into training and test sets amounts to peeking into the future and will provide too optimistic estimates of model performance. This is therefore not a suitable evaluation strategy. In situations like this, one uses historical data to train a model and predicts data from the current period. To simulate this scenario you will use the most recent (last) 4000 samples for testing and everything else for training.\n",
    "\n",
    "Train a Random Forests or Extra Trees classifier on the training set and evaluate it on the test set using an appropriate performance metric. The selection of metric should take into account whether the classes are balanced or not, as well as the goal of the prediction task. Do you think your model would be useful in practice? Why or why not?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) This is a somewhat open ended problem. There is no single correct answer.\n",
    "\n",
    "2) The data set is described in some detail here:\n",
    "https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "and a previous attempt at predictive modeling in: https://www.sciencedirect.com/science/article/pii/S016792361400061X\n",
    "\n",
    "3) To convert the data into a matrix format suitable for scikit-learn, it is probably easiest to use the Python Data Analysis Library (pandas) package. You load the data using\n",
    "\n",
    "`import pandas as pd\n",
    "bank_df=pd.read_csv('bank-additional-full.csv',sep=';')`\n",
    "\n",
    "You can iterate over the features using e.g.\n",
    "\n",
    "`for col in bank_df.columns:\n",
    "    if bank_df[col].dtype == object:\n",
    "        print(\"Categorical: \",col)\n",
    "    else:\n",
    "        print(\"Numerical: \", col)`\n",
    "\n",
    "The output variable (`y`) is 1 if a customer subscribes and 0 otherwise. \n",
    "\n",
    "Start by using only the numerical data. Then add the categorical data gradually. More data does not always help.\n",
    "\n",
    "The simplest conversion of categories to integers is called label encoding. In pandas:\n",
    "\n",
    "`bank_df['y']=bank_df['y'].astype('category')\n",
    "bank_df['y']=bank_df['y'].cat.codes\n",
    "y=bank_df['y'].values\n",
    "bank_df=bank_df.drop('y',axis=1) # Remove output variable`\n",
    "\n",
    "or using scikit-learn instead:\n",
    "\n",
    "`from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(bank_df['y'])[:,0]`\n",
    "\n",
    "Label encoding of a feature assumes that the feature values have a natural ordering (are ordinal). This has some drawbacks as detailed in the lecture notes and one-hot-encoding is generally preferred. This is most conveniently done by using `pd.get_dummies` with `drop_first=True`. For this particular data set, direct application of one-hot-encoding does not necessarily improve performance.\n",
    "\n",
    "4) In addition to `sklearn.metrics.confusion_matrix` which can be used to derive sensitivity, specificity and accuracy, the `sklearn.metrics.classification_report` class provides performance metrics called precision recall and F-score (see Wikipedia for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
