{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 6\n",
    "### Due: Monday 25.2.2019\n",
    "\n",
    "**Objectives**: Boosting and stacking algorithms in supervised learning\n",
    "\n",
    "**Name**: (your name here), **email: ** (your email here), **collaborators:** (if any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. [AdaBoost classifier, 50 points]\n",
    "\n",
    "a) [40 points] Implement the AdaBoost algorithm described on page 339 in ESL using small decision trees as base classifiers. Redo the computations for the example of Figure 10.2. Plot the training error as well as test error, and discuss its behavior.\n",
    "\n",
    "Use the following data set. The features $x_1^{(i)},\\ldots,x_{10}^{(i)}$ are standard independent normally distributed variables and the output is defined as $y^{(i)}=1$ if $\\sum_{j=1}^{10} (x^{(i)}_j)^2 > 9.34$ and zero otherwise (see comments below).\n",
    "\n",
    "b) [10 points] Repeat the experiments from a) using a decision tree that captures 2-way feature interactions (3 leaf nodes). How do the results differ from a)?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) Use `sklearn.tree.DecisionTreeClassifier` to generate the decision trees. Tree stumps are obtained by setting `max_leaf_nodes=2`. The behaviour of your ensemble classifier may be somewhat different from the one shown in Figure 10.2 in ESL since the implementation of the base tree classifier is different. For this reason you can use e.g. 1000 boosting iterations instead of 400.\n",
    "\n",
    "2) The `DecisionTreeClassifier.fit` function accepts a vector of sample weights as an optional argument.\n",
    "\n",
    "3) The training data set in a) is obtained with the function `hw6` below with `hw6(n=1000)`. The test sets are obtained in the same way but using $n=10000$.\n",
    "\n",
    "4) If $\\text{err}_m$ becomes zero we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hw6(n=1000):\n",
    "    # Toy dataset from page 339 in ESL\n",
    "    X=np.random.randn(2*n,10)\n",
    "    y=2*(np.sum(X**2,axis=1)>9.34)-1\n",
    "    return X,y\n",
    "\n",
    "# Your solution comes here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2\\. [Stacked regression models, 50 points] In this problem you will construct a stacked two-stage regression model for a subset of the Million Song Database (MSD). The data set contains audio features for approximately 500K songs. Each song is represented by 90 features describing its \"timbre\" that are derived from the sampled recordings. The task is to predict the release year of a song.\n",
    "\n",
    "A two-stage stacking model has several regression models in stage 1, all trained on the same data set. Predictions from stage 1 models form a new (derived) data set which is used as input to a single regression model in stage 2. This model \"blends\" predictions from the stage 1 models to create a final prediction, hopefully more accurate than the individual stage 1 predictions.\n",
    "\n",
    "Your stacked regression model will employ Lasso, ExtraTrees, Random Forests and Gradient boosted trees in stage 1 and a linear regression model in stage 2. Training and testing are performed as follows:\n",
    "\n",
    "*Training*: Train each model on the training set, using default parameters to begin with, but increase the number of trees for Extra Trees and Random Forests. Construct a training data set for the stage 2 model by sending the *validation* set (not the original training set) through each of the stage 1 models, resulting in an `n_val` by 4 matrix $X_2$. Train a linear regression model for stage 2 on $(X_2, y_\\text{val})$.\n",
    "\n",
    "*Testing*: Send the test data though all the models in stage 1 to obtain an `n_test` by 4 matrix. The stage 2 linear regression model is used to predict the data in this matrix to obtain the final predictions.\n",
    "\n",
    "a) [32 points] Construct the stacked regression model described above, report its mean-squared error and $R^2$ coefficient on the test set. Report also the mean-squared error of the individual stage 1 models.\n",
    "\n",
    "b) [8 points] Answer the following questions:\n",
    "\n",
    "i) Are the individual models doing a good job on the prediction task? Why or why not?\n",
    "\n",
    "ii) Is the stacking procedure worth the extra effort in your opinion? Why or why not?\n",
    "\n",
    "iii) Are any of the regression models sensitive to scaling of input data?\n",
    "\n",
    "iv) Why is it not a good idea to use the original training set to construct the $X_2$ data set for the stage 2 regression model?\n",
    "\n",
    "c) [10 points] In the spirit of Kaggle, can you improve the results from a) by tuning hyperparameters in level 1, using a different regression model in stage 2 or more training data?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) Download the subset of the Million Song Databse from here (210 MB): http://archive.ics.uci.edu/ml/datasets/YearPredictionMSD# (mirror: https://notendur.hi.is/steinng/kennsla/2019/ml/data/YearPredictionMSD.zip)\n",
    "\n",
    "2) Use the train, validation and test partitions of the data defined in `load_msd.py`\n",
    "\n",
    "`import load_msd as lmsd\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = lmsd.get_data(ntrain=10000)`\n",
    "\n",
    "3) For Extra Trees and Random Forests you can set `n_jobs=-1` to use multiple cores/processors for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your solution comes here\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
