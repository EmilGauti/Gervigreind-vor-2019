{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 7\n",
    "### Due: *Monday* 4.3.2019\n",
    "\n",
    "**Objectives**: Visualization with principal component analysis and t-SNE. $k$-means clustering\n",
    "\n",
    "**Name**: (your name here), **email: ** (your email here), **collaborators:** (if any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. [Visualization, 40 points] In this problem you will use PCA and t-SNE to visualize a high-dimensional data set derived from 300 Wikipedia articles selected from few broad groups of topics. For each Wikipedia article, the most common words such as 'an' and 'the' were removed and the rest of the words run through a stemming algorithm (converting e.g. 'longer' and 'longest' to 'long'). This resulted in a dictionary of all the words that occur in the 300 articles. The total number of words was 1000. A 1000-element histogram vector was then constructed for each article, where element $j$ is the frequency of word $j$ in the document, i.e. a 300 by 1000 matrix.\n",
    "\n",
    "The Numpy file `wikipedia_corpus.npz` contains three arrays which you access as follows\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data=np.load('wikipedia_corpus.npz')\n",
    "dictionary = data[\"dictionary\"]\n",
    "article_titles = data[\"article_titles\"]\n",
    "article_histograms = data[\"article_histograms\"] # Data matrix```\n",
    "\n",
    "a) [20 points] Use PCA to create a 2D figure where each article in the figure is represent by a short string based on its title.\n",
    "\n",
    "b) [10 points] Use the t-SNE code provided with the assignment to create a similar figure to the one in a). You may need to try a few different values of the perplexity parameter before you get a nice projection (include only the best one in your report). Can you \"squeeze\" more titles into this figure than the one in a)? What can you infer from your visualization?\n",
    "\n",
    "c) [10 points] Use t-SNE to project the data to 3 dimensions. Use the first two dimensions as text coordinates but use the 3rd coordinate to color code the article title. Is this figure more informative than the one in b) in your opinion?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) Creating informative figures usually takes some effort so expect to spend some time tinkering with your figure. See http://www.cs.toronto.edu/~hinton/turian.png for an example of how your figure could look like.\n",
    "\n",
    "2) You should try to use as large figure as possible, use `plt.figure(figsize=(xsize,ysize))`\n",
    "\n",
    "3) You can only display titles of 100 - 150 articles in the figure, otherwise you are likely to end up with a black mess.\n",
    "\n",
    "4) Some of the titles are quite long and you should therefore truncate them somehow, e.g. by keeping only the two first words in the title. Useful Python's string `split` and `join` methods may come in handy. Use `plt.text` to display text in the figure.\n",
    "\n",
    "5) For the color figure in c) see e.g. https://lvdmaaten.github.io/tsne/examples/semantic_tsne.jpg\n",
    "\n",
    "6) A simple way to convert coordinate values into a color value is to map them into an integer betwen 0 and 9 and use the `color=Cx` keyword argument in `plt.text` (x represents an integer between 0 and 9).\n",
    "\n",
    "7) Use PCA from scikit and the t-SNE code provided with this assignment (taken from https://lvdmaaten.github.io/tsne/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. [Topic discovery via $k$-means, 30 points] Here you are to use the $k$-means algorithm to cluster the data from problem 1.\n",
    "\n",
    "Run $k$-means with different values of $k$, e.g. $k=2,5,8$ and investigate your results by looking at the words and article titles associated with each centroid. Feel free to visit Wikipedia if an article’s content is unclear from its title. On the basis of your tests, select a final value of $k$ and run $k$-means again. Give a short description of the topics your clustering discovered along with the 5 most common words from each topic. If the topics do not make sense pick another value of $k$.\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) When you run the $k$-means implementation in `sklearn.cluster.KMeans` it initializes the centroids by randomly assigning the data points to $k$ groups and taking the $k$ representatives as the means of the groups. (This means that if you run the function twice, with the same data, you might get diﬀerent results.) The cluster centers and labels can be accessed via the attributes `cluster_centers_` and `labels_`. The attribute `labels_` contains the index of each vector’s closest centroid (labels start from zero), so if the 30th entry in `labels` is 7, then the 30th vector’s closest centroid is the 7th entry in `centroids` (indexing starts from zero).\n",
    "\n",
    "2) There are many ways to explore your results. For example, you could print the titles of all articles in a cluster. Alternatively, you could ﬁnd a topic’s most common words by ordering `dictionary` by the size of its centroid’s entries. A larger entry for a word implies it was more common in articles from that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3\\. [Image compresssion with $k$-means, 30 points] In this problem which is from Andrew Ng at Stanford, you \n",
    "will apply the $k$-means algorithm to lossy image compression, by reducing the number of colors used in an image.\n",
    "You will be using the files `mandrill-small.tiff` and `mandrill-large.tiff`.\n",
    "\n",
    "The `mandrill-large.tiff` file contains a 512x512 image of a mandrill represented in 24-\n",
    "bit color. This means that, for each of the 262144 pixels in the image, there are three 8-bit\n",
    "numbers (each ranging from 0 to 255) that represent the red, green, and blue intensity\n",
    "values for that pixel. The straightforward representation of this image therefore takes\n",
    "about 262144×3 = 786432 bytes (a byte being 8 bits). To compress the image, we will use\n",
    "$k$-means to reduce the image to $k = 16$ colors. More specifically, each pixel in the image is\n",
    "considered a point in the three-dimensional $(r, g, b)$-space. To compress the image, we will\n",
    "cluster these points in color-space into 16 clusters, and replace each pixel with the closest\n",
    "cluster centroid.\n",
    "\n",
    "Follow the instructions below.\n",
    "\n",
    "To load the image, type\n",
    "```python\n",
    "   from matplotlib.image import imread.\n",
    "   import matplotlib.pyplot as plt\n",
    "   A = imread('mandrill-large.tiff')```\n",
    "\n",
    "Now, `A` is a \"three dimensional\" matrix, and `A[:,:,0]`, `A[:,:,1]` and `A[:,:,2]` are $512 \\times 512$ arrays\n",
    "that respectively contain the red, green and blue values for each pixel. To display the image, enter\n",
    "```python\n",
    "   plt.imshow(A);\n",
    "   plt.show() to display the image```\n",
    "\n",
    "Note: The `imshow` function expects the red, green and blue values to be between 0 and 1. You should start by scaling all  $(R,G,B)$ values by dividing by 255, otherwise you may end up with incorrect color scheme in the reconstructed image below.\n",
    "\n",
    "Since the large image has 262144 pixels and would take a while to cluster, we will instead\n",
    "run vector quantization on a smaller image. Repeat the above with `mandrill-small.tiff`.\n",
    "Treating each pixel’s $(r, g, b)$ values as an element of $R^3$. Run $k$-means with 16 clusters\n",
    "on the pixel data from this smaller image.\n",
    "\n",
    "Take the matrix `A` from `mandrill-large.tiff`, and replace each pixel’s $(r, g, b)$ values\n",
    "with the value of the closest cluster centroid. Display the new image, and compare it\n",
    "visually to the original image.\n",
    "\n",
    "If we represent the image with these reduced (16) colors, by (approximately) what\n",
    "factor have we compressed the image?\n",
    "\n",
    "*Comment*: Use the $k$-means implementation in `sklearn.cluster.KMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
