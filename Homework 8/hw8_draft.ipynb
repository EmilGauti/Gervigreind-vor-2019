{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 8 (**UNDER CONSTRUCTION!!!**)\n",
    "### Due: *Monday* 11.3.2019\n",
    "\n",
    "**Objectives**: Topic discovery with NMF, Image compression with PCA and NMF, Spectral clustering\n",
    "\n",
    "**Name**: Emil Gauti Friðriksson, **email: ** egf3@hi.is, **collaborators:** (if any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. [*Topic discovery with NMF*, 40 points]. Here you will use non-negative matrix factorization (NMF) to analyze the content of tweets from Donald Trump. In particular, you will attempt to discover the main topics of his tweets by applying NMF to a document-term matrix derived from the tweets (or rather to a \"tweet-term\" matrix).\n",
    "\n",
    "The NMF approximates a non-negative $n \\times p$ matrix $X$ of rank $r$ with a rank $k \\leq r$ matrix such that\n",
    "\n",
    "$$\n",
    "X \\approx WH\n",
    "$$\n",
    "\n",
    "where $W$ is a $n \\times k$ matrix with $W_{ij} \\geq 0$ and $H$ is a $k \\times p$ matrix with $H_{ij} \\geq 0$. Provided that $k$ is appropriately chosen, the *weight matrix* $W$ and *coefficient matrix* $H$ can reveal interesting structures in the data. Column $j$ of $X$ is approximated with (see comment 1 below)\n",
    "\n",
    "$$\n",
    "X_{:,j} \\approx (WH)_{:,j} = H_{1j}W_{:,1} + H_{2j}W_{:,2} + \\ldots + H_{kj}W_{:,k}\n",
    "$$\n",
    "\n",
    "where the subscript $:,j$ denotes column $j$. The columns of $W$ in this context correspond to the main topics of Trump's tweets and column $j$ of $H$ contains information on how the topics are \"mixed\" together to form (approximately) column $j$ of $X$.\n",
    "\n",
    "a) Download all tweets by Trump from http://www.trumptwitterarchive.com/archive from the period 20.1.2017 (inauguration day) to present, omitting retweets, as a CSV file (approx. 5800 tweets). Create a tweet-term matrix using word counts (see below). For a given value of $k$, perform NMF on the matrix and list the words corresponding to the largest $H_{ij}$ values for columns $j=1,\\ldots,k$. You need to experiment with different values of $k$ (a.k.a. the *Trump-dimension*) to get interesting topic groupings. If $k$ is too low different topics will be mixed together, when $k$ gets large, the same subject will appear in multiple clusters. Report your results (c.a. 20 words on each topic) for the value of $k$ that you end up picking.\n",
    "\n",
    "b) Select two topics of \"interest\" (e.g. Trump's nemesis Hillary Clinton). Identify the corresponding columns in $W$ and list approx. 5 tweets using the largest $W$-values as indices. Does the content of the tweets match the selected topics?\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) The $n \\times k$ matrix-vector product $y=Ax$ can be interpreted as a weighted sum of the columns of $A$,\n",
    "$$\n",
    "y=\n",
    "\\begin{array}{ccc}\n",
    "~\\mid &  & ~\\mid \\\\\n",
    "x_1 a_1 & + \\ldots + & x_k a_k \\\\\n",
    "~\\mid & & ~\\mid \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "and matrix multiplication can be considered as multiple matrix-vector products.\n",
    "\n",
    "2) Use the NMF implementation in`from sklearn.decomposition.NMF`. You can use the Wikipedia data set from HW7 to test your NMF-based topic discovery code. Once you get convincing results, apply your code to the newly constructed tweet-term matrix.\n",
    "\n",
    "3) Use `sklearn.feature_extraction.text.CountVectorizer` to create the document-term matrix based on word counts from the raw tweets. This function performs tokenization, counting and normalization and removes stop words. Use the following parameter values `max_features=k`, `max_df=0.95` (remove words that occur in at least 95% of the documents), `min_df=2` (remove words that occur in fewer than two documents), `stop_words='english'`.\n",
    "\n",
    "4) Use `CountVectorizer.get_feature_names()` to get the list of words that were retained. Sidenote: Rare words are downplayed by the term-frequency encoding used here but they are often found to be informative. Therefore people often encode the text using term-frequency-inverse document frequency.\n",
    "\n",
    "5) Scikit's NMF function obtaines the factorization $X \\approx WH$ by minimizing the objective function $0.5||X - WH||_F^2$ (here $||A||_F$ denotes the Frobenius norm of a matrix $A$, $||A||_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n A_{ij}^2}$. The NMF implementation provides means to regularize the solution via parameters `alpha` and `l1_ratio`. You may want to experiment with these parameters to see if you can improve the list of topics.\n",
    "\n",
    "6) The $H$ matrix is stored in `nmf.components_`\n",
    "\n",
    "7) The NMF is described briefly in section 14.6 of ESL. A more detailed account can be found in the original article\n",
    "http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flokkur nr 1 : amp repeal military replace taxes years women going said nation getting economy work hard dems healthcare doj men isis loves \n",
      "flokkur nr 2 : people country american want way years enemy come like coming going millions working time million laws let history know media \n",
      "flokkur nr 3 : news fake media cnn just story stories don bad reporting said dishonest enemy nbc new big house good like report \n",
      "flokkur nr 4 : border wall security democrats want country don southern crime immigration stop need drugs republicans laws dems mexico national military open \n",
      "flokkur nr 5 : fbi collusion witch hunt hillary democrats russia clinton crooked campaign comey just russian election rigged mueller angry hoax dossier investigation \n",
      "flokkur nr 6 : president trump obama donald thank election campaign administration did just xi said russia american foxandfriends right day years china isis \n",
      "flokkur nr 7 : big crime vote tax military strong endorsement vets win cuts borders total jobs amendment loves governor 2nd senate state republicans \n",
      "flokkur nr 8 : great america make state job thank honor today doing new governor day congratulations country senator night healthcare work meeting wonderful \n",
      "flokkur nr 9 : https thank today honor america american whitehouse welcome jobs join day national nation americans united families vote world minister melania \n",
      "flokkur nr 10 : trade korea china north united just states deal good years time country tariffs countries new dollars year meeting kim jobs \n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "def print_top_words(nmf,feature_names, n_top_words):\n",
    "    for nr, topic in enumerate(nmf.components_,1):\n",
    "        top_words_indx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        ordd = ''\n",
    "        for i in range(n_top_words):\n",
    "            ordd += feature_names[top_words_indx[i]]+' '\n",
    "        print('flokkur nr', nr, ':', ordd)\n",
    "\n",
    "\n",
    "data = np.genfromtxt('trump_tweets.CSV',encoding=\"utf-8\", delimiter=',',dtype=str)\n",
    "k=10\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95,min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(data)#þessi gæji heldur utan um hve oft orðin koma fyrir\n",
    "\n",
    "nmf = NMF(n_components=k,init='random', random_state=0,alpha=.1, l1_ratio=.5).fit(X)\n",
    "\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "print_top_words(nmf, feature_names, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "Ég vel flokkana sem mér sýnist tengjast **Border Wall**(flokkur nr. 4) og **Fake News**(flokkur nr. 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5742, 10)\n",
      "[0.         0.00409233 0.         ... 0.         0.00094659 0.00089892]\n",
      "[   0 3179 3180 ... 1746 1117 1741]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-3a1185d1f87d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtop_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "#Select two topics of \"interest\" (e.g. Trump's nemesis Hillary Clinton). Identify the corresponding columns in  WW  and list approx. \n",
    "#5 tweets using the largest  WW -values as indices. Does the content of the tweets match the selected topics?\n",
    "W = nmf.transform(X)\n",
    "print(W.shape)\n",
    "#print(np.sum(W[:,3]))\n",
    "#print(np.sum(W[:,4]))\n",
    "bw_fn = [2,3]\n",
    "n_tweets = 5\n",
    "for ind in bw_fn:\n",
    "    weight = W[:,ind]\n",
    "    print(weight)\n",
    "    top_tweets = np.argsort(weight)\n",
    "    print(top_tweets)\n",
    "    print(top_tweets[:,3])\n",
    "    print(W[0,ind])\n",
    "    for i in range(n_tweets):\n",
    "        print('maff',W[top_tweets[i,ind],ind])\n",
    "        print(data[top_tweets[i]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. [*Image compresssion with PCA and NMF*, 30 points] Fit a non-negative matrix factorization model to the zero-digits in the subset of the MNIST database from the Jupyter workbook `v07_pca_tsne_kmeans` (download from Piazza). Perform the following using 25 basis elements in the factorization:\n",
    "\n",
    "i) Display the $W$ matrix as an image (see Fig. 14.33 in ESL) as well as an image for the part of $H$ that corresponds to the first image in the data set.\n",
    "\n",
    "ii)  Compare a reconstruction of the first image in the data set with the original image. What compression ratio is achieved with 25 basis elements?\n",
    "\n",
    "b) Repeat the analysis in a using 24-component (plus mean) PCA model (see Fig. 14.33 in ESL). Compare briefly with the results in a)\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) Use the NMF implementation in `sklearn.decomposition.NMF`. The *columns* of the input matrix should contain the pixel values for each image (this is opposed to how we treated image data earlier). The `fit_transform` function returns the $W$ matrix and the attribute `components_` contains the $H$ matrix.\n",
    "\n",
    "2) When reconstructing images you may need to \"clip\" the data, i.e. set pixel values above 1.0 to 1.\n",
    "\n",
    "3) Many elements of the $W$ matrix will be zero and when you use a gray-scale color map, these elements will show up as black. You might therefore want to represent positive values with black and zeros with white.\n",
    "\n",
    "4) Use scikit to perform PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. [*Spectral clustering*] Under construction!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
