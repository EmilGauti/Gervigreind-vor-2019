{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REI602M Machine Learning - Homework 9\n",
    "### Due: *Monday* 18.3.2019\n",
    "\n",
    "**Objectives**: The PageRank algorithm, Feedforward neural networks, backpropagation, mini-batch gradient descent.\n",
    "\n",
    "**Name**: Emil Gauti Friðriksson, **email: ** egf3@hi.is, **collaborators:** (if any)\n",
    "\n",
    "**Note**: Problems 2 and 3 are taken from Andrew Ng's Machine learning course at Stanford. Several solutions to them have been made available on the web but I trust that you will not look them up and solve the problems on your own (feel free to collaborate though). The third problem will take some time, but when you do complete it you will have acquired a solid understanding on training feedforward neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. [PageRank, 30 points] In this problem you implement a simple version of the PageRank algorithm.\n",
    "<img src=\"simple_web.png\" width=\"200\">\n",
    "Construct the normalized connectivity matrix $Q$ for the above graph where nodes represent web pages and directed edges represent hyperlinks, $Q_{ij}=1/N_j$ if there is a link from $j$ to $i$ and $N_j$ is the number of outlinks from page $j$. Let $e$ denote a vector of ones and $d_j=1$ if $N_j=0$ but zero otherwise. Calculate the matrices $P=Q+(1/n)ed^T$ and $A=\\alpha P + (1-\\alpha)(1/n)ee^T$ for $\\alpha=0.85$. Report $Q,P$ and $A$. The ranking $r$ is given by the largest eigenvector of $A$. Compute $r$ by implementing the *power method*, starting from $r^{0}=(1/n)1$ and using $\\epsilon=10^{-4}$.\n",
    "\n",
    "Iterate $k=1,2,\\ldots$\n",
    "\n",
    "$\\quad q^{(k)} = Ar^{(k-1)}$\n",
    "\n",
    "$\\quad r^{(k)}=q^{(k)} / ||q^{(k)}||_1$\n",
    "\n",
    "until $||r^{(k)} - r^{(k-1)}|| < \\epsilon $.\n",
    "\n",
    "Report the rank of each of the nodes in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Q:\n",
      "[[0.         1.         0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.5        0.33333333 0.         0.         0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.5        0.         0.         0.         0.         0.        ]\n",
      " [0.5        0.         0.         0.33333333 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.         0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.         1.         0.        ]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A:\n",
      "[[0.01875    0.86875    0.01875    0.01875    0.86875    0.125      0.01875    0.01875   ]\n",
      " [0.01875    0.01875    0.44375    0.30208333 0.01875    0.125      0.01875    0.01875   ]\n",
      " [0.44375    0.01875    0.01875    0.01875    0.01875    0.125      0.01875    0.01875   ]\n",
      " [0.01875    0.01875    0.44375    0.01875    0.01875    0.125      0.01875    0.01875   ]\n",
      " [0.44375    0.01875    0.01875    0.30208333 0.01875    0.125      0.01875    0.01875   ]\n",
      " [0.01875    0.01875    0.01875    0.30208333 0.01875    0.125      0.01875    0.01875   ]\n",
      " [0.01875    0.01875    0.01875    0.01875    0.01875    0.125      0.01875    0.86875   ]\n",
      " [0.01875    0.01875    0.01875    0.01875    0.01875    0.125      0.86875    0.01875   ]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "P:\n",
      "[[0.         1.         0.         0.         1.         0.125      0.         0.        ]\n",
      " [0.         0.         0.5        0.33333333 0.         0.125      0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.125      0.         0.        ]\n",
      " [0.         0.         0.5        0.         0.         0.125      0.         0.        ]\n",
      " [0.5        0.         0.         0.33333333 0.         0.125      0.         0.        ]\n",
      " [0.         0.         0.         0.33333333 0.         0.125      0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.125      0.         1.        ]\n",
      " [0.         0.         0.         0.         0.         0.125      1.         0.        ]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "   node        r     \n",
      "----------------------\n",
      "    a       0.578623 \n",
      "    g       0.405465 \n",
      "    h       0.405465 \n",
      "    e       0.361819 \n",
      "    c       0.307383 \n",
      "    b       0.246444 \n",
      "    d       0.192008 \n",
      "    f       0.115729 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "alpha=0.85\n",
    "n=8#fjöldi hnúta\n",
    "Q = np.zeros((n,n))\n",
    "# Create the dictionary with graph elements\n",
    "graph = { \"a\" : [\"e\",\"c\"],\n",
    "          \"b\" : [\"a\"],\n",
    "          \"c\" : [\"b\", \"d\"],\n",
    "          \"d\" : [\"e\",\"b\",\"f\"],\n",
    "          \"e\" : [\"a\"],\n",
    "          \"f\" : [],\n",
    "          \"g\" : [\"h\"],\n",
    "          \"h\" : [\"g\"]\n",
    "         }\n",
    "abc = ['a','b','c','d','e','f','g','h']\n",
    "\n",
    "for i,val1 in enumerate(abc):\n",
    "    for j,val2 in enumerate(abc):\n",
    "        if val1 in graph[val2]:\n",
    "            Q[i,j] = 1/len(graph[val2])\n",
    "\n",
    "d = np.array([0,0,0,0,0,1,0,0])\n",
    "e = np.ones(n)\n",
    "\n",
    "P = Q + (1/n)*e*d.T\n",
    "A = alpha*P+(1-alpha)*(1/n)*e*e.T\n",
    "\n",
    "def r_func(A,n):\n",
    "    r = 1/n*np.ones(n)\n",
    "    eps = 10e-4\n",
    "    error = 1\n",
    "    while error>eps:\n",
    "        q = A@r\n",
    "        r_new=q/np.linalg.norm(q)\n",
    "        error = np.linalg.norm(r-r_new)\n",
    "        r=r_new\n",
    "    return(r)\n",
    "\n",
    "r = r_func(A,n)\n",
    "\n",
    "np.set_printoptions(linewidth=400)\n",
    "print('-'*100)\n",
    "print('Q:')\n",
    "print(Q)\n",
    "print('-'*100)\n",
    "print('A:')\n",
    "print(A)\n",
    "print('-'*100)\n",
    "print('P:')\n",
    "print(P)\n",
    "print('-'*100)\n",
    "print('-'*100)\n",
    "\n",
    "print('{:^10s} {:^10s}'.format('node','r'))\n",
    "print('-'*22)\n",
    "for i in range(n):\n",
    "    print('{:^10s} {:^10f}'.format(abc[np.argsort(r)[::-1][i]],r[np.argsort(r)[::-1][i]] ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. [Neural networks, 30 points] Let $X=\\{(x^{(1)}, \\ldots, x^{(n)}\\}$ be a dataset of $n$ samples with 2-dimensional features, i.e. $x^{(i)} \\in R^2$, that are classified into two categories with labels $ y \\in \\{0,1\\}$. The data set is shown below.\n",
    "\n",
    "<img src=\"prob2data.png\" width=\"300\">\n",
    "\n",
    "The examples in class 1 are marked as \"x\" and examples in class 0 are marked as \"0\". The task is to perform binary classification using a simple neural network with the architecure shown below.\n",
    "\n",
    "<img src=\"prob2network.png\" width=\"200\">\n",
    "\n",
    "Denote the two features $x_1$ and $x_2$, the three neurons in the hidden layer, $h_1,~h_2,~h_3$, and the output neuron as $o$. Let the weight from $x_i$ to $h_j$ be $w_{i,j}^{[1]}$ for $i\\in\\{1,2\\},~j \\in \\{1,2,3\\}$, and the weight from $h_j$ to $o$ be $w_j^{[2]}$. Finally denote the intercept weight for $h_j$ as $w_{0,j}^{[1]}$ and the intercept weight for $o$ as $w_{0}^{[2]}$. The averaged squared loss will be used as a loss function, instead of the usual negative log-likelihood,\n",
    "$$\n",
    "\\ell = \\frac{1}{n} \\sum_{i=1}^n(o^{(i)} - y^{(i)})^2\n",
    "$$\n",
    "where $o^{(i)}$ is the result of the output neuron for example $i$.\n",
    "\n",
    "a) [10 points] Suppose that the sigmoid function is used as the activation function for $h_1,~h_2,~h_3$ and $o$. What is the gradient descent update to $w_{1,2}^{[1]}$ assuming that a learning rate of $\\alpha$ is used? Your answer should be written in terms of $x^{(i)},~o^{(i)},y^{(i)}$ and the weights.\n",
    "\n",
    "b) [10 points] Now, suppose instead of using the sigmoid function for the activation function for $h_1,~h_2,h_3$ and $o$, the step function $f(x)$ is used instead, defined as\n",
    "$$\n",
    "f(x) = \\left\\{ \\begin{array}{cc} 1&x \\geq 0 \\\\ 0&x < 0 \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "What is one set of weights that would allow the neural network to classify this dataset with 100% accuracy? Please specify a value for the weights in the order given below and explain your reasoning.\n",
    "$$\n",
    "w_{0,1}^{[1]} = ?,~ w_{1,1}^{[1]} = ?,~w_{2,1}^{[1]} = ?\n",
    "$$\n",
    "$$\n",
    "w_{0,2}^{[1]} = ?,~ w_{1,2}^{[1]} = ?,~w_{2,2}^{[1]} = ?\n",
    "$$\n",
    "$$\n",
    "w_{0,3}^{[1]} = ?,~ w_{1,3}^{[1]} = ?,~w_{2,3}^{[1]} = ?\n",
    "$$\n",
    "$$\n",
    "w_{0}^{[2]} = ?,~ w_{1}^{[2]} = ?,~w_{2}^{[2]} = ?,~~w_{3}^{[2]} = ?\n",
    "$$\n",
    "\n",
    "*Hint*: The are three sides to a triangle, and there are three neurons in the hidden layer.\n",
    "\n",
    "c) [10 points] Let the activation functions for $h_1,~h_2,~h_3$ be the linear function $f(x)=x$ and the activation function for $o$ be the same step function as before. Is there a specific set of weights that will make the loss 0? If yes, please explicitly state a value for every weight. If not, please explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. [Neural network classification of the MNIST data set, 40 points] In this problem, you will implement a simple neural network to classify grayscale images of handwritten digits (0 - 9) from the MNIST dataset. The dataset contains 50,000 training images, 10,000 validation images and 10,000 testing images of handwritten digits, 0 - 9, represented as a vector of $28 \\times 28 = 784$ elements. Download the dataset from https://notendur.hi.is/steinng/kennsla/2019/ml/data/hw9_mnist.zip\n",
    "\n",
    "To start, you will implement a neural network with a single hidden layer and cross entropy loss, and train it with the MNIST data set. Use the sigmoid function as activation for the hidden layer, and the softmax function (see comments below) for the output layer.\n",
    "\n",
    "<img src=\"prob3network.png\" width=\"400\">\n",
    "\n",
    "The cross entropy loss for a single example $(x, y)$ and $K$ classes is\n",
    "$$\n",
    "CE(y,\\hat{y}) = -\\sum_{k=1}^K y_k \\log \\hat{y}_k\n",
    "$$\n",
    "where $\\hat{y}\\in R^k$ is the vector of softmax outputs from the model for training example $x$ and $y \\in R^K$ is the ground truth vector for training example $x$ such that $y=[0,\\ldots,0,1,0,\\ldots,0]^T$ contains a single 1 at the position of the correct class (this is sometimes referred to as a \"one-hot\" representation).\n",
    "\n",
    "For $n$ training examples, the cross entropy loss is averaged over the $n$ examples,\n",
    "$$\n",
    "J(W^{[1]}, W^{[2]}, b^{[1]}, b^{[2]}) = \\frac{1}{n} \\sum_{i=1}^n CE(y^{(i)}, \\hat{y}^{(i)})\n",
    "= -\\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^k y_k^{(i)} \\log \\hat{y_k}{(i)}.\n",
    "$$\n",
    "\n",
    "The starter code below already converts labels into one hot representations for you.\n",
    "\n",
    "Instead of batch gradient descent or stochastic gradient descent, common practice is to use mini-batch gradient descent for deep learning tasks. In this case, the cost function is defined as follows,\n",
    "$$\n",
    "J_{MB} = \\frac{1}{B} \\sum_{i=1}^B CE(y^{(i)},\\hat{y}^{(i)})\n",
    "$$\n",
    "where $B$ is the number of training examples in the mini-batch.\n",
    "\n",
    "a) [25 points] Implement both forward-propagation and back-propagation for the above loss function. Initialize the weights of the network by sampling values from a standard normal distribution, e.g. $N(0,0.1)$. Initialize the bias/intercept term to 0. Set the number of hidden units to be 300, and learning rate to be 5. Set $B = 1000$ (mini batch size). This means that you train with 1,000 examples in each iteration. Therefore, for each epoch, 50 iterations are needed to cover the entire training data. The images are pre-shuffled. So you don’t need to randomly sample the data, and can just create mini-batches sequentially.\n",
    "Train the model with mini-batch gradient descent as described above. Run the training for 30 epochs. At the end of each epoch, calculate the value of loss function averaged over the entire training set, and plot it ($y$-axis) against the number of epochs ($x$-axis). In the same image, plot the value of the loss function averaged over the validation set, and plot it against the number of epochs.\n",
    "Similarly, in a new image, plot the accuracy (on $y$-axis) over the training set, measured as the fraction of correctly classified examples, versus the number of epochs ($x$-axis). In the same image, also plot the accuracy over the validation set versus number of epochs.\n",
    "Also, at the end of 30 epochs, save the learnt parameters (i.e all the weights and biases) into a file, so that next time you can directly initialize the parameters with these values from the file, rather than re-training all over. You do NOT need to hand in these parameters.\n",
    "\n",
    "b) [10 points] Now add a regularization term to your cross entropy loss. The loss function\n",
    "will become\n",
    "$$\n",
    "J_{MB} = \\frac{1}{B} \\sum_{i=1}^B CE(y^{(i)},\\hat{y}^{(i)}) + \\lambda (||W^{[1]}||^2 + ||W^{[2]}||^2)\n",
    "$$\n",
    "Be careful not to regularize the bias/intercept term (see section 7.1 in the Deep learning book). Set $\\lambda$ to be 0.0001. Implement the regularized version and plot the same figures as part (a). Be careful NOT to include the regularization term to calculate the loss value (regularization should only be used to calculate the gradients). Submit the two new plots obtained.\n",
    "Compare the plots obtained from the regularized version with the plots obtained from the non-regularized version, and summarize your observations in a couple of sentences. As in the previous part, save the learnt parameters (weights and biases) into a different file so that we can initialize from them next time.\n",
    "\n",
    "c) [5 points] All this while you should have stayed away from the test data completely.\n",
    "Now that you have convinced yourself that the model is working as expected, it is finally time to measure the model performance on the test set. Once we measure the test set performance, we report it whatever value it may be, and NOT go back and refine the model any further.\n",
    "\n",
    "Initialize your model from the parameters saved in part (a) (i.e, the non-regularized model), and evaluate the model performance on the test data. Repeat this using the parameters saved in part (b) (i.e, the regularized model). Report your test accuracy for both regularized model and non-regularized model.\n",
    "\n",
    "*Hint*: Be sure to vectorize your code as much as possible! Training can be very slow otherwise.\n",
    "\n",
    "*Comments*:\n",
    "\n",
    "1) Andrej Karpathy explains why you should understand the backpropagation algorithm: https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n",
    "\n",
    "2) The softmax function is a generalization of the logistic function for $K$ classes. See https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n",
    "3) When debugging your code, use a subset of the data and start e.g. with a 2-class problem. For information on how to check the gradient calculations and other sanity checks, see http://cs231n.github.io/neural-networks-3/#gradcheck\n",
    "\n",
    "4) Yann Le Cun reports a 4.7% error rate for a similar network (using mean square error instead of cross-entropy) so you should not be disappointed if your network fails to achieve superhuman performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starter code and utility functions for problem 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def readData(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "    return x, y\n",
    "\n",
    "def writeData(fname, X):\n",
    "    np.savetxt(fname, X, delimiter=',')\n",
    "    return\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for input. \n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    ### END YOUR CODE\n",
    "    return s.T\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    ### END YOUR CODE\n",
    "    return s\n",
    "\n",
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    return hidder layer, output(softmax) layer and loss\n",
    "    \"\"\"\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "  \n",
    "    ### YOUR CODE HERE\n",
    "    ### END YOUR CODE\n",
    "    return h, y, cost\n",
    "\n",
    "def backward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    return gradient of parameters\n",
    "    \"\"\"\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "  \n",
    "    ### YOUR CODE HERE\n",
    "    ### END YOUR CODE\n",
    "  \n",
    "    grad = {}\n",
    "    grad['W1'] = gradW1.T\n",
    "    grad['W2'] = gradW2.T\n",
    "    grad['b1'] = gradb1.T\n",
    "    grad['b2'] = gradb2.T\n",
    "  \n",
    "    return grad\n",
    "\n",
    "def nn_train(trainData, trainLabels, devData, devLabels):\n",
    "    (m, n) = trainData.shape\n",
    "    m, K = trainLabels.shape\n",
    "    num_hidden = 300\n",
    "    learning_rate = 5\n",
    "    params = {}\n",
    "  \n",
    "    ### YOUR CODE HERE\n",
    "    ### END YOUR CODE\n",
    "  \n",
    "    return params\n",
    "\n",
    "def nn_test(data, labels, params):\n",
    "    h, output, cost = forward_prop(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    accuracy = (np.argmax(output,axis=1) == np.argmax(labels,axis=1)).sum() * 1. / labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "images_train.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-10abf73722c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# (this takes a while but you only need to do this once)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrainLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-d9181da4857f>\u001b[0m in \u001b[0;36mreadData\u001b[0;34m(images_file, labels_file)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    616\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    617\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: images_train.csv not found."
     ]
    }
   ],
   "source": [
    "# Read the data, construct a validation set and do some basic preprocessing\n",
    "# (this takes a while but you only need to do this once)\n",
    "np.random.seed(100)\n",
    "trainData, trainLabels = readData('images_train.csv', 'labels_train.csv')\n",
    "trainLabels = one_hot_labels(trainLabels)\n",
    "p = np.random.permutation(60000)\n",
    "trainData = trainData[p,:]\n",
    "trainLabels = trainLabels[p,:]\n",
    "\n",
    "devData = trainData[0:10000,:]\n",
    "devLabels = trainLabels[0:10000,:]\n",
    "trainData = trainData[10000:,:]\n",
    "trainLabels = trainLabels[10000:,:]\n",
    "\n",
    "mean = np.mean(trainData)\n",
    "std = np.std(trainData)\n",
    "trainData = (trainData - mean) / std\n",
    "devData = (devData - mean) / std\n",
    "\n",
    "testData, testLabels = readData('images_test.csv', 'labels_test.csv')\n",
    "testLabels = one_hot_labels(testLabels)\n",
    "testData = (testData - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check that the data was read correctly\n",
    "import matplotlib.pyplot as plt\n",
    "print(trainData[10].shape)\n",
    "plt.imshow(trainData[10].reshape(28,28),cmap=\"gray\") # Should be a \"2\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You need to fill in the code above before executing this part\n",
    "params = nn_train(trainData, trainLabels, devData, devLabels)\n",
    "\n",
    "readyForTesting = True\n",
    "if readyForTesting:\n",
    "    accuracy = nn_test(testData, testLabels, params)\n",
    "print('Test accuracy: %f' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
