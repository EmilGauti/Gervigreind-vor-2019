{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stigulsútreikningar í þjálfun tauganeta\n",
    "\n",
    "Eftirfarandi dæmi sýnir stigulsútreikninga fyrir einfalt tauganet og minnstu kvaðrata markfall. Tauganetið hefur tvær inntaksnóður og eina úttaksnóðu (engar huldar nóður). Ennfremur er sýnt hvernig hægt er að sannreyna stigulsútreikninga með því að nálga stigulinn tölulega með svokallaðri mismunaaðferð (e. finite difference method). Í henni er afleiða falls $f$ af einni breytistærð, $w \\in \\mathbb{R}$ nálguð með\n",
    "$$\n",
    "f'(w) \\approx \\frac{f(w+h)-f(w-h)}{2h}\n",
    "$$\n",
    "þar sem $h>0$ er einhver lítil tala. Á hliðstæðan hátt fást hlutafleiður fyrir föll af mörgum breytistærðum, $w \\in \\mathbb{R}^q$,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial w_j} = \\approx \\frac{f(x+he_j)-f(x-he_j)}{2h}\n",
    "$$\n",
    "þar sem $e_j$ er einingavigur samsíða $j$-ta ás hnitakerfisins (stak $j$ er 1, öll önnur stök eru 0). Athugið að það þarf að reikna $2q$ fallsgildi til að nálga stigulinn í einum punkti. Backpropagation aðferðin er talsvert hagkvæmari og er þess vegna mikið notuð við þjálfun tauganeta.\n",
    "\n",
    "Flestum for-lykkjum í kóðanum er hægt að skipta út fyrir vektoraðgerðir til að flýta útreikningum. Ennfremur er verið að reikna sömu stærðirnar óþarflega oft.\n",
    "\n",
    "Nánar: http://cs231n.github.io/neural-networks-3/#gradcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def g(z):\n",
    "    # Activation function (sigmoid)\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def dg(z):\n",
    "    # Derivative of the sigmoid activation function\n",
    "    return g(z)*(1-g(z))\n",
    "\n",
    "def J(w,X,y):\n",
    "    # Least squares objective function\n",
    "    s=0\n",
    "    for i in range(X.shape[0]):\n",
    "        s+=(y[i]-g(np.dot(w.T,X[i,:])))**2\n",
    "    return s\n",
    "\n",
    "def gradJ(w,X,y):\n",
    "    # Exact gradient\n",
    "    n,p = X.shape\n",
    "    grad=np.zeros(p)\n",
    "    for i in range(n):\n",
    "        for j in range(p):\n",
    "            grad[j]+=2*(y[i]-g(np.dot(w.T,X[i,:])))*(-dg(np.dot(w.T,X[i,:])))*X[i,j] # Not optimized at all!\n",
    "    return grad\n",
    "\n",
    "def approx_grad(w,X,y,h=1e-4):\n",
    "    # Finite difference approximation of the gradient of f (central differences)\n",
    "    # Note: If h becomes very small, round-off errors will dominate\n",
    "    p=len(w)\n",
    "    grad=np.zeros(p)\n",
    "    for j in range(p):\n",
    "        e = np.zeros((p,1))\n",
    "        e[j] = 1\n",
    "        grad[j]=(J(w + e*h, X,y) - J(w - e*h, X,y))/(2*h)\n",
    "    return grad\n",
    "\n",
    "# Toy data set\n",
    "X=np.array([[1,2],[2,5]])\n",
    "y=np.array([0,1])\n",
    "w=np.array([-3.1,-0.23]).reshape(2,1) # Fix weights to some arbitrary values\n",
    "\n",
    "print(\"Exact gradient: \", gradJ(w, X,y))\n",
    "print(\"Approximate gradient:\", approx_grad(w,X,y,1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eftirfarandi dæmi sýnir hvernig hægt er að þjálfa einfalt tauganet með aðferð mesta bratta þar sem mismunaaðferðin er notuð til að nálga stigulinn. Netið hefur 2 inntaksnóður, eitt hulið lag með 3 nóðum og eina úttaksnóðu. Markfallið er minnsta kvaðratskekkja.\n",
    "\n",
    "Athugið að útreikningar hafa ekki verið \"optimeraðir\" með tilliti til hraða. Backpropagation myndi svo gefa mikinn tímasparnað."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN training using gradient descent and numerical approximation of the gradient\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Model parameters W1, W2, b1, b2 are stacked in vector w\n",
    "# NB Hard coding of matrix/vector dimensions is an ugly hack!\n",
    "def pack_vector(W1,W2,b1,b2):\n",
    "    return np.vstack((W1.reshape(6,1), W2.reshape(3,1), b1, b2))\n",
    "\n",
    "def unpack_vector(w):\n",
    "    W1=w[0:6].reshape(3,2)\n",
    "    W2=w[6:9].reshape(1,3)\n",
    "    b1=w[9]\n",
    "    b2=w[10]\n",
    "    return W1, W2, b1, b2\n",
    "    \n",
    "def g(z):\n",
    "    return 1/(1+np.exp(-z)) # Sigmoid activation function\n",
    "\n",
    "def predict(w,x):\n",
    "    W1,W2,b1,b2=unpack_vector(w)\n",
    "    # Feedforward operation\n",
    "    z1=np.dot(W1,x.reshape(2,1)) + b1\n",
    "    a1=g(z1) # Layer 1 activation\n",
    "    z2=np.dot(W2,a1) + b2\n",
    "    a2=g(z2) # Layer 2 activation\n",
    "    return a2\n",
    "\n",
    "def J(w,X,y):\n",
    "    # Least squares objective function\n",
    "    s=0\n",
    "    for i in range(X.shape[0]):\n",
    "        s +=(y[i]-predict(w,X[i,:]))**2\n",
    "    return s\n",
    "\n",
    "def approx_grad(f,w,X,y,h=1e-4):\n",
    "    p=len(w)\n",
    "    grad=np.zeros((p,1))\n",
    "    for j in range(p):\n",
    "        e = np.zeros((p,1))\n",
    "        e[j] = 1\n",
    "        grad[j]=(f(w + e*h, X,y) - f(w - e*h, X,y))/(2*h)\n",
    "    return grad\n",
    "\n",
    "# 2D data set from homework 3\n",
    "data=np.genfromtxt('../heimaverkefni/hw3/hw3_data_a.txt')\n",
    "X=data[:,1:]\n",
    "y=data[:,0]\n",
    "y[y==-1]=0\n",
    "\n",
    "# NN architeture: 2 inputs, 3 hidden nodes, 1 output\n",
    "# Initialization\n",
    "sigma=0.1\n",
    "W1=sigma*np.random.randn(3,2)\n",
    "W2=sigma*np.random.randn(1,3)\n",
    "b1=sigma*np.random.randn(1)\n",
    "b2=sigma*np.random.randn(1)\n",
    "\n",
    "# Batch gradient descent\n",
    "alpha=0.1\n",
    "w=pack_vector(W1,W2,b1,b2) # Model parameters\n",
    "for k in range(1,1001):\n",
    "    if np.mod(k,100) == 0:\n",
    "        print(\"Iter:\", k, \"obj=\", J(w,X,y))\n",
    "    dJ=approx_grad(J,w,X,y,1e-4)\n",
    "    w = w - alpha*dJ\n",
    "print(\"Final weights\")\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the decusion boundary \n",
    "import matplotlib.pyplot as plt\n",
    "plot_colors = 'rby'\n",
    "plot_step = 0.1 # Grid density\n",
    "n_classes= len(np.unique(y))\n",
    "x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "Xgrid=np.c_[xx.ravel(), yy.ravel()]\n",
    "Z=np.zeros(Xgrid.shape[0])\n",
    "for i in range(Xgrid.shape[0]):\n",
    "    Z[i]=predict(w,Xgrid[i])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
