{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Földunarnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def readData(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "    return x, y\n",
    "\n",
    "np.random.seed(100)\n",
    "trainData, trainLabels = readData('../heimaverkefni/hw9/images_train.csv',\n",
    "                                  '../heimaverkefni/hw9/labels_train.csv')\n",
    "p = np.random.permutation(60000)\n",
    "trainData = trainData[p,:]\n",
    "\n",
    "# One-hot encoding of class labels\n",
    "trainLabels = tf.keras.utils.to_categorical(trainLabels, num_classes=10)\n",
    "trainLabels = trainLabels[p,:]\n",
    "\n",
    "# Take 10000 training samples aside for validation (a.k.a. \"dev\" set)\n",
    "devData = trainData[0:10000,:]\n",
    "devLabels = trainLabels[0:10000,:]\n",
    "trainData = trainData[10000:,:]\n",
    "trainLabels = trainLabels[10000:,:]\n",
    "\n",
    "# Remove mean from data and scale it to speed up training\n",
    "mean = np.mean(trainData)\n",
    "std = np.std(trainData) # a scalar value\n",
    "trainData = (trainData - mean) / std\n",
    "devData = (devData - mean) / std\n",
    "\n",
    "testData, testLabels = readData('../heimaverkefni/hw9/images_test.csv',\n",
    "                                '../heimaverkefni/hw9/labels_test.csv')\n",
    "testLabels = tf.keras.utils.to_categorical(testLabels, num_classes=10)\n",
    "testData = (testData - mean) / std\n",
    "\n",
    "# Convert data to 32-bit floating point representation\n",
    "# This saves memory and speeds up training of deep networks\n",
    "# and is *essential* for utilizing GPUs for training.\n",
    "trainData = trainData.astype('float32')\n",
    "testData = testData.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Dense#, Activation\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras.optimizers import Adadelta\n",
    "\n",
    "num_epochs = 2 # Increase to get improved accuracy\n",
    "batch_size = 128\n",
    "\n",
    "img_rows, img_cols = 28, 28 # input image dimensions\n",
    "num_classes = 10\n",
    "\n",
    "x_train = trainData.reshape(trainData.shape[0], img_rows, img_cols, 1)\n",
    "y_train = trainLabels\n",
    "\n",
    "x_val = devData.reshape(devData.shape[0], img_rows, img_cols, 1)\n",
    "y_val = devLabels\n",
    "\n",
    "x_test = testData.reshape(testData.shape[0], img_rows, img_cols, 1)\n",
    "y_test = testLabels\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=num_epochs,\n",
    "                  verbose=1,\n",
    "                  validation_data=(x_val, y_val))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Convolutional neural network')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endurkvæm net\n",
    "\n",
    "Sýnidæmið hér að neðan er dæmi um \"sequence to sequence\" lærdóm með endurkvæmu tauganeti. Netið lærir samlagningu tveggja talna. Inntak er strengur á borð við \"123+45\" og úttakið \"168\". Netið er þjálfað á nokkur þúsund dæmum sem útbúin eru af handahófi. Netið er síðan prófað á dæmum sem það hefur ekki séð áður."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last output of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for iteration in range(1, 200):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)\n",
    "\n",
    "    # Evaluate performance on the validation set\n",
    "    #corr=0\n",
    "    #for ind in range(len(x_val)):\n",
    "    #    rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "    #    preds = model.predict_classes(rowx, verbose=0)\n",
    "    #    q = ctable.decode(rowx[0])\n",
    "    #    correct = ctable.decode(rowy[0])\n",
    "    #    guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "    #    if correct == guess:\n",
    "    #        corr+=1\n",
    "    #print(\"Validation accuracy:\", corr/len(x_val))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
